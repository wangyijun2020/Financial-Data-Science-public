{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Worksheet: Maximum Likelihood for a Nonlinear Problem\n",
    "\n",
    "Dear students,\n",
    "\n",
    "My team and I tried hard to get a useful exercise for MLE. MLE is a crucial tool for all data science. In addition, our standard problem set was handed out as part of the python lecture notes. We hence had to come up with something useful, yet, also doable at the BSc level.\n",
    "\n",
    "The following exercise is the result. We will likely spend some time discussing your and my work during the upcoming prof cafe.\n",
    "\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remember this Crucial FACT\n",
    "\n",
    "**Do NOT numerically optimize a Gaussian likelihood for a LINEAR model. Instead, take the ML parameter estimates from an OLS fit AND the ML estimate for the volatility from the respective residual**\n",
    "\n",
    "$$\n",
    "\\sqrt{\\frac{1}{T} \\epsilon_{ols}' \\epsilon_{ols}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  A Useful Exercise had to be Non-Linear\n",
    "\n",
    "Reason: see above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## G.0 Set-up of the Python Challenge\n",
    "\n",
    "Have you ever wondered how central banks, banks, hedge funds and investors know whether a government bond is fairly priced, overpriced or undervalued? Well, they use the approach of \"net present value\" to figure out what the price of a bond should be. To do so, one has to get accurate forecasts of how the short rate and risk premia move into the future. A popular approach relies on so called \"short rate models\". These models fit the time series of the short rate to a realistic, yet convenient, parametrization. This allows to obtain a forecast of where short rates will be in the future. You need to do the same for the risk premium. You might wonder why do government bonds pay a risk premium. Well, the longer the maturity of the bond, the higher the interest rate sensitivity and hence, the higher (usually) the (maturity) premium. That premium compensates for the fact that future realized interest rates will be different from todays expected future rates. Based on the CAPM, you could say that interest rate risk is systematic and hence pays a risk premium. That is for the background.\n",
    "\n",
    "A very popular short rate model is called \"Vasicek model\". That is because a mathematician, Dr. Vasicek, derived in the 1980s the term structure of interest rates under the assumption that the short rate follows a Gaussian distribution. It is fair to says that the assumed short rate follows a G-LRM. Based on stochastic calculus and the principle of net present value and no arbitrage, Dr. Vasicek derived the price of risk-free (non defaultable) bonds for different maturities. \n",
    "\n",
    "The Gaussian PDF of the short rate is\n",
    "\n",
    "$$\n",
    "r_t | F_{t-1} \\sim \\mathcal N\\left(\\theta^P (1- e^{-\\kappa^P})+e^{-\\kappa^P} r_{t-1}, \\sigma^2 * \\frac{1-e^{-2\\kappa^P}}{2\\kappa^P}\\right).\n",
    "$$\n",
    "\n",
    "Note, $\\theta^P$ is the long-run mean of $r$, $\\kappa^P$ is the speed of mean reversion and $\\sigma$ is the instantaneous standard deviation of the interest rate shock. As the model of Vasicek is written in continuous time, the above PDF takes several exponentials and ratios, which are the result of an integration that is NOT subject to our BSc course.\n",
    "\n",
    "Also, Dr. Vasicek derived that the arbitrage-free price of a bond at time $t$ with maturity in $\\tau$ periods, $P_t(\\tau)$ coincides with\n",
    "\n",
    "$$\n",
    "P_t(\\tau) = e^{A(\\tau) - B(\\tau) \\times r_t}\n",
    "$$\n",
    "\n",
    "with\n",
    "\\begin{align*}\n",
    "B(\\tau) &:= \\frac{1-e^{-\\kappa^Q \\tau}}{\\kappa^Q} \\\\\n",
    "A(\\tau) &:= (\\theta^Q - \\frac{\\sigma^2}{2 (\\kappa^Q)^2}) \\times (  B(\\tau) - \\tau ) - \\frac{\\sigma^2}{4\\kappa^Q} B^2(\\tau) \n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "Note, from the CAPM and NPV you know that prices of financial assets are discounted by the risk-free rate plus risk premium. The parameters $\\theta^Q$ and $\\kappa^Q$ differ from $\\theta^P$ and $\\kappa^P$, respectively. The former are adjusted by risk premiums. Intuitively, it is correct to think that the former are risk premium adjusted expressions of the latter. E.g. if the long-run mean of the short rate was at 2\\%, it could be that bonds are priced as if the long-run rate was at 3\\%. This means the price is lower than if the future payoff was discounted by 2\\%. That extra reduction in price can be correctly interpreted as the result of a risk premium. That was to show you that the so called \"Q parameters\" are risk premium adjusted (and extracted from prices), while the $P$ parameters are extracted from empirical time-series data. I provide a formal and intuitive introduction into these asset pricing concepts in an upcoming MSc class on financial machine learning.\n",
    "请注意，根据 CAPM 和 NPV，您知道金融资产的价格按无风险利率加上风险溢价进行贴现。 参数 𝜃𝑄 和 𝜅𝑄 分别不同于 𝜃𝑃 和 𝜅𝑃 。 前者通过风险溢价进行调整。 直觉上，认为前者是后者的风险溢价调整表达式是正确的。 例如。 如果短期利率的长期平均值为 2%，则债券的定价可能与长期利率为 3%。 这意味着价格低于未来收益贴现 2% 的情况。 价格的额外下降可以正确解释为风险溢价的结果。 那是为了向您展示所谓的“Q 参数”是经过风险溢价调整的（并从价格中提取），而 𝑃 参数是从经验时间序列数据中提取的。 我在即将到来的金融机器学习理学硕士课程中对这些资产定价概念进行了正式而直观的介绍。\n",
    "\n",
    "For now, you need only to keep in mind that the parameters of the model are $\\kappa^P, \\theta^P$ for the short rate PDF and $\\theta^Q, \\kappa^Q$ for the bond prices. The parameter $\\sigma$ affects both, the short rate PDF and the bond price PDF.\n",
    "\n",
    "Note, prices are non-stationary! Hence, we work with continuousy compounded yields\n",
    "\\begin{align*}\n",
    "y_t(\\tau) &:= - \\frac{1}{\\tau} \\ln P_t(\\tau)  \n",
    "\\end{align*}\n",
    " \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## G.1 Data\n",
    "\n",
    "You will work with the same interest rate data than a couple of weeks ago. The rates are in \"GovBondYields.xls\". \n",
    "\n",
    "Take the 12-, 60 and 120 months interest rates as your yields of interest, $y$. Take the 3-month yield as the short rate $r$.\n",
    "\n",
    "**Units:** Divide the rates data by 1200 to arrive at monthly values in decimals. Work with these units.\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## G.2 Financial Data Science Challenge\n",
    "\n",
    "Your data science task is to extract the time-series of the expected (interest rate) risk premium that is priced-into government bonds. \n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## G.3 Battle Plan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The battle plan is as follows. \n",
    "\n",
    "$$\n",
    "\\\\\n",
    "$$\n",
    "\n",
    "1. Write down the joint log likelihood function for $r_t$ and $y_t(\\tau), \\tau \\in [12,60,120]$. For that assume a pairwise independent Gaussian measurement error for all yields. Further assume each of these measurement errors shares the same volatility, which we call\n",
    "$$\n",
    "\\sigma_y.\n",
    "$$\n",
    "Note, $\\sigma_y$ will be part of the parameter vector over which you optimize.\n",
    "\n",
    "Note, you can interpret observed yields as Vasicek (model)-implied yield $y_t(\\tau)$ plus a Gaussian measurement error that is drawn from $N(0,\\sigma^2_y)$.\n",
    "\n",
    "Note: the above assumption implies that the joint log likelihood function can be treated as the sum of 4 pairwise independent log likelihood functions. The independence refers to the respective residual being independent of the residuals of the other log likelihood. The parameters do of course affect all of the log likelihoods; although strictly speaking, $\\kappa^P, \\theta^P$ affect only the log likelihood for $r$, while $\\kappa^Q, \\theta^Q$ affect only the log likelhood for the 3 yields.\n",
    "\n",
    "\n",
    "$$\n",
    "\\\\\n",
    "$$\n",
    "\n",
    "2. Write a python function that implements the joint log likelihood function. More recent research has shown you could indeed fit the PDF for $r$ independently for the PDF of $y$. Yet, here, we optimize the joint log likelihood.\n",
    "\n",
    "**Hint:** Multivariate Gaussian likelihood: https://en.wikipedia.org/wiki/Multivariate_normal_distribution\n",
    "\n",
    "yet, yours will look simpler as we made some simplifying assumptions. \n",
    "\n",
    "\n",
    "$$\n",
    "\\\\\n",
    "$$\n",
    "\n",
    "3. Play a bit around with starting values for the optimization. That simple example reveals already, that you need to choose starting values carefully to end up in the global optimum. Regardless of your playing around, use at least the following procedure to find smart starting values\n",
    "\n",
    "3.1 fit $r$ to an OLS and recover the ML estimates for $\\kappa^P, \\theta^P, \\sigma$. Make these parameters be the respective starting values $\\kappa^P, \\theta^P, \\sigma$ in  the joint log likelihood optimization. \n",
    "\n",
    "3.2 for the starting values of the Q parameters, we assume a 0 risk premium. Hence $\\kappa^Q_{startValue} = \\kappa^P_{startValue}$ and $\\theta^Q_{startValue} = \\theta^P_{startValue}$. Double check: inside the optimization, Q and P parameters can differ(!). P parameters affect $r$ while $Q$ parameters affect yields (!)\n",
    "\n",
    "3.3 For the starting value of $\\sigma_y$ we do the following: Regress (OLS) the 60month yield onto $r$ and take the volatility of the residual as the starting value for $\\sigma_y$, i.e. (np.sqrt( (...).mse_resid)) where we use the MSE_RESID function of stats model's OLS package. \n",
    "\n",
    "**Hint (1):** The rounded ols starting values coincide with 0.014895, 0.00466, 0.014895, 0.00466, 0.00037794, 0.000777 for $\\kappa^P$, $\\theta^P$, $\\kappa^Q$, $\\theta_Q$, $\\sigma$, $\\sigma_y$, respectively.\n",
    "\n",
    "**Hint (2):** The negative of the joint log likelihood function (over $r$ and $y$), evaluated at   the OLS starting values coincides with -1305x.xxxx (around -13k) \n",
    "\n",
    "\n",
    "$$\n",
    "\\\\\n",
    "$$\n",
    "\n",
    "4. The optimization routine is similar to the ipynb from class. Yet, you will learn that the local optimizer fails to find parameters that beat the starting values. \n",
    "\n",
    "4.1 use sco.minimize\n",
    "\n",
    "4.2 use L-BFGS-B as the optimization routine\n",
    "\n",
    "4.3 use a 'gtol' and 'ftol* of 1e-12\n",
    "\n",
    "4.4 minimize the negative joint log likelihood function\n",
    "\n",
    "4.6 the upper bound for all parameters is **1**. \n",
    "\n",
    "4.7 the lower bound for all parameters is **-1**, except for $\\sigma$ and $\\sigma_y$ whose lower bound we set to **0.00001**.\n",
    "\n",
    "\n",
    "$$\n",
    "\\\\\n",
    "$$\n",
    "\n",
    "5. That nonlinear problem requires a global optimization routine. Hence, redo the optimization with the following numerical optimizer: \n",
    "\n",
    "from scipy.optimize import dual_annealing\n",
    "\n",
    "results_annealing = dual_annealing(func=likelihood_neg, args=(rate_df, yield_df), bounds=bnds_all, seed=19937, maxiter=2000)\n",
    "\n",
    "where likelihood_neg is the function that computes the negative join log likelihood, args = (...) are the pandas data frames for the short rate $r$ and the 3 yields $y$ that are used to learn the parameters, and bnds_all is a tuple containing the lower and upper bounds as in 4.6 and 4.7. \n",
    "\n",
    "\n",
    "$$\n",
    "\\\\\n",
    "$$\n",
    "\n",
    "\n",
    "6. Estimate the model parameters of Vasicek using the local optimizer from step 4 and the global one from step 5.\n",
    "\n",
    "**Hint:** The optimal joint log likelihood function takes the value (for global optimization) of -1481x.xxxx around (-14.8k). \n",
    "\n",
    "**Praktomat will ask for:** MLE for $\\kappa^P$, $\\theta^P$, $\\kappa^Q$, $\\theta_Q$, $\\sigma$, $\\sigma_y$, respectively of the local and the global optimization routine.\n",
    "\n",
    "\n",
    "$$\n",
    "\\\\\n",
    "$$\n",
    "\n",
    "7. Inside the vasicek model, the priced-in expected risk premium of a $\\tau$ maturity bond, $ERP_{t,\\tau}$ is\n",
    "\n",
    "\\begin{align*}\n",
    "ERP_{t,\\tau} &= -B(\\tau) \\times \\sigma \\times MPR_t \\\\\n",
    "MPR_t &:= \\lambda_0 + \\lambda_1 \\times r_t\n",
    "\\end{align*}\n",
    "\n",
    "with\n",
    "\\begin{align*}\n",
    "\\lambda_0 &:= \\frac{\\kappa^P \\theta^P - \\kappa^Q \\theta^Q}{\\sigma_r} \\\\\n",
    "\\lambda_1 &:= \\frac{\\kappa^Q - \\kappa^P}{\\sigma_r}.\n",
    "\\end{align*}\n",
    "\n",
    "and $B(\\tau)$ being the interest rate loading in the Vasicek derivation of the bond price (i.e. duration of the bond).\n",
    "\n",
    "Here, you see that spread of Q and P parameters captures risk premium information. A formal explanation of that is not doable in the BSc course on Financial Data Science.\n",
    "\n",
    "Plot the time series of the expected risk premium for the 1-yr and 10-yr bond. \n",
    "\n",
    "**Praktomat will ask for:** $ERP_{0,120}$, $ERP_{10,120}$, $ERP_{100,120}$, $ERP_{500,120}$.\n",
    "\n",
    "\n",
    "$$\n",
    "\\\\\n",
    "$$\n",
    "\n",
    "8. Consider the basic return decompostion: $r = \\mu + \\epsilon$. Plot the respective $\\mu$ and $\\epsilon$ for the ten-year bond. Compare it to the return decomposition that you have seen for equity returns.\n",
    "\n",
    "**Hint:**\n",
    "\\begin{align*}\n",
    "\\mu &:= r_t + ERP_{t,\\tau}|_{\\tau = 120} \\\\\n",
    "\\epsilon &:= -B(\\tau)|_{\\tau=120} \\times  \\epsilon^{r}\n",
    "\\end{align*}\n",
    "\n",
    "where $\\epsilon^{r}$ is the realized innovation (i.e. residual) to the short rate. \n",
    "\n",
    "**Hint:** you get $\\epsilon^{r}$ from the time series of $r_t$ and its respective Gaussian PDF.\n",
    "\n",
    "**Hint on Hint:** Take the MLE implied interest rate innovation (residual) and you found the time series of $\\epsilon^r$.\n",
    "\n",
    "Note: Ensure you see why a surprise increase in the short rate $\\epsilon^{r}>0$ leads to a negative bond return (i.e. higher discount rate than expected, lower price than expected).\n",
    "\n",
    "\n",
    "**Praktomat will ask for:** $\\mu[0], \\mu[1], \\mu[100], \\mu[500]$ and $\\epsilon[0], \\epsilon[1], \\epsilon[100], \\epsilon[500]$.\n",
    "\n",
    "$$\n",
    "\\\\\n",
    "$$\n",
    "\n",
    "9. As you plot the ten-year bond yield in data and the vasicek implied ten-year bond yield (from the global optimiztion) you will notice that the one factor model of vasicek has problems fitting longer-term yields. This happens because Vasicek uses only one factor to explain bond yields, while one needs at least two factors (better three) to explain movements in short-, medium- and long-term bond yields. A generalization to three factors is not the scope of this worksheet, yet, it is straight-forward to do so, if you have successfully completed this worksheet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import statsmodels.api as sm \n",
    "import scipy.optimize as sco\n",
    "from scipy.optimize import differential_evolution \n",
    "from scipy.optimize import dual_annealing \n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Atau_Btau(params):\n",
    "    kappa_p, theta_p, kappa_q, theta_q, sigma_r, sigma_y= params \n",
    "    Btau =(1 - np.exp(-kappa_q * tau))/ kappa_q\n",
    "    Atau =(theta_q - (sigma_r ** 2)/(2. * kappa_q ** 2)) *(Btau-tau)-sigma_r ** 2. * np.square(Btau)/(4. *kappa_q)\n",
    "    \n",
    "    return Atau,Btau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yield_model_impl(params):\n",
    "    Atau,Btau = Atau_Btau(params=params)\n",
    "\n",
    "    rate = rate_df.values\n",
    "    Pt = np.exp(Atau - Btau * rate)\n",
    "    yt_model_impl = -np.log(Pt)/ tau\n",
    "    yield_obs = yield_df.values\n",
    "    yield_error=(yield_obs-yt_model_impl)\n",
    "    return yt_modeL_impl, yield_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#likelihood function\n",
    "\n",
    "def Likelihood_neg(params,rate_df,yield_df):\n",
    "    kappa_p = params [0]\n",
    "    theta_p = params[1] \n",
    "    kappa_q= params [2] \n",
    "    theta_q = params [3] \n",
    "    sigma_r = params [4]\n",
    "    sigma_y = params[5] \n",
    "    \n",
    "    rate = rate_df.values \n",
    "    T = len(rate)\n",
    "    # mu^P_t\n",
    "    residuals= rate[1:T]-(rate[0:T-1] * np.exp(-kappa_p)) + theta_p * (1-np.exp(-kappa_p))\n",
    "    #Sigma2\n",
    "                          \n",
    "    sigma2t_ll = sigma_r ** 2*(1 -np.exp(-2* kappa_p))/(2 * kappa_p)\n",
    "    \n",
    "    r_p1 = -1 / 2 * np. log(2 * np.pi) \n",
    "    r_p2 = -1 / 2 * np. log(sigma2t_ll)\n",
    "    r_p3 = -1 / 2 * np. square(residuaLs)/ sigma2t_ll\n",
    "    Likeli_rates = r_p1 + r_p2 + r_p3 \n",
    "    Atau,Btau = Atau_Btau( params=params)\n",
    "    # tmp2 = Btau[0]*rate[0]\n",
    "    # tmp = Btau*rate\n",
    "    Pt = np.exp(Atau - Btau * rate)\n",
    "    yt_model_impl = -np.log(Pt)/ tau\n",
    "    yield_obs = yield_df.values\n",
    "    yield_error=(yield_obs-yt_model_impl)\n",
    "   \n",
    "    #dim(y_t)\n",
    "    k = yield_df.shape[1]\n",
    "    \n",
    "    Y_p1 = -k / 2 * np.Log(2 *np.pi) \n",
    "    Y_p2= -k /2*np.log(sigma_y ** 2) \n",
    "    Y_p3 = -1/2 * np.square(yield_error) /(sigma_y **2) \n",
    "    llk_yields = y_p1 +y_p2+y_p3\n",
    "                          \n",
    "    llk_all=np.sum(likeli_rates) + np.sum(llk_yields)\n",
    "    llk_all_neg= -1* llk_all\n",
    "                          \n",
    "    return llk_all_neg\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def ln_likelihood_r(params, r) :\n",
    "    theta = params[0]\n",
    "    kappa = params[1]\n",
    "    sigma = params[2]\n",
    "\n",
    "    T = r.shape[0]\n",
    "    sigma2_short = (sigma**2 / (2 * kappa)) * (1 - np.exp(-2 * kappa))\n",
    "    term1 = -(T/2) * np.log(2*np.pi)\n",
    "    term2 = -(T/2) * np.log(sigma2_short)\n",
    "\n",
    "    constant = theta * (1 - np.exp(-kappa))\n",
    "    beta_1 = np.exp(-kappa)\n",
    "\n",
    "    mu_short = constant + beta_1 * r[:(T-1), :]\n",
    "    eps = r[1:] - mu_short\n",
    "    term3 = (eps.T @ eps)[0,0]\n",
    "    term3 = -0.5 * term3 / sigma2_short\n",
    "\n",
    "    return term1 + term2 + term3\n",
    "\n",
    "def A(tau, theta, kappa, sigma) :\n",
    "    term1 = theta - (sigma**2 / (2 * kappa**2))\n",
    "    term2 = B(tau, kappa) - tau\n",
    "    term3 = (sigma**2 / (4 * kappa)) * (B(tau, kappa)**2)\n",
    "    return term1 * term2 - term3\n",
    "def B(tau, kappa) :\n",
    "    return (1/kappa) * (1 - np.exp(-kappa * tau))\n",
    "def a(tau, theta, kappa, sigma) :\n",
    "    return A(tau, theta, kappa, sigma) * (-1/tau)\n",
    "def b(tau, kappa) :\n",
    "    return B(tau, kappa) * (1/tau)\n",
    "def ln_likelihood_y(params, tau, r, y, sigma) :\n",
    "    theta = params[0]\n",
    "    kappa = params[1]\n",
    "    sigma_y = params[2]\n",
    "    T = r.shape[0]\n",
    "\n",
    "    term1 = -(T/2) * np.log(2*np.pi)\n",
    "    term2 = -(T/2) * np.log(sigma_y ** 2)\n",
    "\n",
    "    mu_y = a(tau,theta,kappa,sigma) + b(tau, kappa) * r\n",
    "    eps = y - mu_y\n",
    "    term3 = (eps.T @ eps)[0,0]\n",
    "    term3 = (-0.5 * term3) / (sigma_y**2)\n",
    "    return term1 + term2 + term3\n",
    "def ln_joint_likelihood(params, r_df, ys_df, taus) :\n",
    "    # The first three parameters are for r and the rest are for y\n",
    "    ln_L = ln_likelihood_r(params[:len(taus)], r_df.to_numpy())\n",
    "    for i in range(len(taus)) :\n",
    "        y_matrix = ys_df.iloc[:, i].to_numpy()[np.newaxis].reshape(ys_df.shape[0], 1)\n",
    "        y_ll = ln_likelihood_y(params[len(taus):], taus[i], r_df.to_numpy(), y_matrix, params[2]) \n",
    "        ln_L += y_ll\n",
    "    return -1 * ln_L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpret_result(results, rate_df,yield_df):\n",
    "    print('Status : %s'%  results['message']) \n",
    "    print('Total Evaluations: %d' % results['nfev'])\n",
    "# evaluate solution \n",
    "    solution_= results ['x']\n",
    "    kappa_p,theta_p,kappa_q,theta_q,sigma_r,sigma_y = solution_ \n",
    "    evaluation_= Likelihood_neg(solution_, rate_df,yield_df)\n",
    "    print(\"kappa_p:{:.6f},theta_p:{:.6f},kappa_q:{:.6f),theta_q:(:.6f),sigma_r:{:.6f),sigma_y:{.6f},kappa_q, theta_q sigma_r,Sigma_y\")\n",
    "    print('Solution: f(%s)= %.6f'%(solution_,evaluation_))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define maturities\n",
    "tau = np.array([12,60,120])## bounds,if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
